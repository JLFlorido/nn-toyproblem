import torch
import torch.nn.functional as F
from torch import nn
import math
import numpy as np
import time
#18 August, third try:
#Going to feed it data in epochs using the equation and then use code from:
#https://medium.com/inspired-ideas/learning-a-quadratic-equation-with-pytorch-intro-to-pytorch-fa7bcef380a4
#to test it


class JNet(nn.Module):
    def __init__(self):
        super().__init__()
        #Define network layers
        self.layers = nn.Sequential(
            nn.Linear(1,6),
            nn.ReLU(),
            nn.Dropout(p=0.5),
            nn.Linear(6,6),
            nn.ReLU(),
            nn.Linear(6,1)
        )

    def forward(self, x):
        #Forward pass
        x = torch.sigmoid(self.layers(x))
        return x

def data_generator(data_size=50):
    # f(x) = y = 8x^2 + 4x - 3
    inputs = []
    labels = []

    # loop data_size times to generate the data
    for ix in range(data_size):
        
        # generate a random number between 0 and 1000
        x = np.random.randint(1000) / 1000
        
        # calculate the y value using the function 8x^2 + 4x - 3
        y = np.cos(x)
        
        # append the values to our input and labels lists
        inputs.append([x])
        labels.append([y])
        
    return inputs, labels


#Instantiate model
model = JNet()

#loss function and optimizer
loss_fn = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)

#
nb_epochs = 10
data_size = 1000
tic=time.time()

#Main training loop
for epoch in range(nb_epochs):
#     #forward pass first
    X, y = data_generator(data_size)

    epoch_loss = 0

    for ix in range(data_size):
        y_pred = model (torch.autograd.Variable(torch.Tensor(X[ix])))
    # Compute and print loss.
        loss = loss_fn(y_pred, torch.autograd.Variable(torch.Tensor(y[ix]), requires_grad=False))   #differnece between ground truth and estimate from forward pass
    
        #epoch_loss = loss.data[0]
        epoch_loss = loss.item()
    #     #The step I don't understand
        optimizer.zero_grad()   #Alternatively: model.zero_grad()
    #     #Backward pass
        loss.backward()         #Uses chain rule to obtain gradients
    #     #Optimizer update
        optimizer.step()        #Updates weights using gradient of loss

    if epoch % 10 == 0:           #This simply prints the loss every 100 iterations
        print("Epoch: {}   Loss: {}".format(epoch, epoch_loss))
print("Final Loss: {}".format(epoch_loss))
model.eval()
test_data = data_generator(1)
prediction = model(torch.autograd.Variable(torch.Tensor(test_data[0][0])))

toc = time.time()

print("Prediction: {}".format(prediction.data[0]))
print("Expected: {}".format(test_data[1][0]))
print(str.format('{0:.3f} sec Elapsed', (toc-tic)))